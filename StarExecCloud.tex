\documentclass{easychair}

% \usepackage{doc}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage[straightquotes]{newtxtt}

%----Suppress extra space in texttt mode
\AddToHook{cmd/ttfamily/after}{\frenchspacing}

%----Making things more compact
\newcommand{\smalltt}[1]{\small \texttt{#1}}
\newenvironment{packed_itemize}{
\vspace*{-0.3em}
\begin{itemize}
\setlength{\partopsep}{0pt}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
}{\end{itemize}}
\newenvironment{packed_enumerate}{
\vspace*{-0.3em}
\begin{enumerate}
\setlength{\partopsep}{0pt}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
}{\end{enumerate}}
% \renewcommand{\textfraction}{0.07}
% \renewcommand{\topfraction}{0.9}
% \renewcommand{\bottomfraction}{0.9}
% \renewcommand{\floatpagefraction}{0.66}
% \setlength{\floatsep}{2.0pt plus 2.0pt minus 2.0pt}
% \setlength{\textfloatsep}{5.0pt plus 2.0pt minus 0.0pt}

\newcommand{\alex}[1]{{\color{red}{{#1}}}}

\title{Towards StarExec in the Cloud}

\author{
  David Fuenmayor\inst{1}
\and
  Jack McKeown\inst{2}
\and
  Geoff Sutcliffe\inst{2}
}

\institute{
  University of Bamberg,
  Bamberg, Germany\\
  \email{david.fuenmayor@uni-bamberg.de}
\and
  University of Miami,
  Miami, USA\\
  \email{jam771@miami.edu,geoff@cs.miami.edu}
}

\authorrunning{Fuenmayor, McKeown, Sutcliffe}
\titlerunning{Stars in the Clouds}

\begin{document}
\maketitle

%--------------------------------------------------------------------------------------------------
\begin{abstract}
\end{abstract}
%--------------------------------------------------------------------------------------------------
% Geoff
\section{Introduction}
\label{Introduction}

Automated Theorem Proving (ATP) is concerned with the development and use of tools that automate 
sound reasoning: the derivation of conclusions that follow inevitably from facts.
Automated Theorem Proving (ATP) is at the heart of many computational tasks, in particular for
verification \cite{Har06,HH19} and security \cite{Coo18}.\footnote{%
In AWS -
\href{https://aws.amazon.com/what-is/automated-reasoning/}{aws.amazon.com/what-is/automated-reasoning/}, 
\href{https://aws.amazon.com/security/provable-security//}{aws.amazon.com/security/provable-security/}.} 
New and emerging application areas include
chemistry \cite{Yad17}, 
biology \cite{CC+13}, 
medicine \cite{HLB05},
elections \cite{Nip09,BDS17}, 
auctions \cite{CK+15}, 
privacy \cite{Lib20},
law \cite{PS15}, 
ethics \cite{DF+16}, 
religion \cite{OZ11,BW14-ECAI,Hor19},
and business \cite{Han98}.
ATP systems are also used as components of more complex Artificial Intelligence (AI) systems,
and the impact of ATP is thus extended into many facets of society.
% in areas such as 
% knowledge representation \cite{TR+04}, 
% natural language processing \cite{BM05}, 
% planning \cite{NV07}, 
% agents \cite{TBP03}, 
% commonsense reasoning \cite{MS05}, 
% and the semantic web \cite{McG04}.

The Thousands of Problems for Theorem Provers (TPTP) World \cite{Sut24} is a well established 
infrastructure that supports research, development, and deployment of ATP systems.
The TPTP World includes 
the TPTP problem library \cite{Sut17},
the TSTP solution library \cite{Sut10},
standards for writing ATP problems and reporting ATP solutions \cite{SS+06,Sut08-KEAPPA},
tools and services for processing ATP problems and solutions \cite{Sut10},
and it supports the the annual CADE ATP System Competition (CASC)~\cite{Sut16}.
Since its first release in 1993 the ATP community has used the TPTP World as an appropriate and 
convenient infrastructure for ATP system development, evaluation, and application.
The TPTP World has a diverse, engaged, and sustained user community, with various parts of the 
TPTP World being deployed in a range of applications in both academia and industry.\footnote{%
TPTP has contributed to recognized research in 627 publications that cite \cite{Sut17},
according to Google Scholar.}
The web page \href{https://www.tptp.org}{\tt www.tptp.org} provides access to all components.

The TPTP problem library was motivated by the need to provide support for meaningful ATP system 
evaluation.
This need was also (or became) evident in other logic solver communities, e.g., 
SAT~\cite{HS00-SATLIB} and SMT~\cite{BST10}.
For many years testing of logic solvers was done on individual developer's computers. 
In 2010 a proposal for centralised hardware and software support was developed,
and in 2011 a \$2.11 million NSF grant\footnote{%
NSF Awards 1058748 and 1058925, led by Aaron Stump and Cesare Tinelli at the University of Iowa} 
was obtained.
This grant led to the development and availability of StarExec Iowa~\cite{SST14} in 2012,
and a subsequent \$1.00 million grant\footnote{%
NSF Award 1730419} in 2017 expanded StarExec to Miami.
StarExec has been central to much progress in logic solvers over the last 10 years, supporting
16 logic solver communities, used for running many annual competitions~\cite{BB+19}, and 
supporting many many users.
StarExec Iowa provides community infrastructure for many logic solver communities,
e.g., ASP, QBF, SAT, SMT, Termination, etc, while
StarExec Miami is used by the TPTP community.
StarExec Miami has features that take advantage of TPTP standards, and is also used to host CASC.

It was recently announced that StarExec Iowa will be decommissioned. 
The maintainer of StarExec Iowa explained that ``the plan is to operate StarExec as usual for 
competitions Summer 2024 and Summer 2025, and then put the system into a read-only mode for one 
year (Summer 2025 to Summer 2026)''.
The 2017 grant for StarExec Miami paid for the hardware and three years of system administration.
The hardware is still hosted by the University of Miami High Performance Computing group,
funded on a shoe-string budget by the TPTP World.
While StarExec Miami will continue to operate while funding is available, it will not be able
to support the large number of logic solver communities that use the larger StarExec Iowa cluster.
In the long term StarExec will necessarily have to migrate to a commonly available compute service.
This paper describes work being done to containerize StarExec and ATP systems, so that they can 
be run on a broad range of computer platforms.
Additionally, this work aims to build a Kubernetes backend in StarExec so that Kubernetes can
be used to orchestrate distribute of StarExec job pairs over whatever compute nodes are available.
One possibility is to host StarExec-Kubernetes in AWS.

\paragraph{This paper is organized as follows:}

%--------------------------------------------------------------------------------------------------
% David
\section{Background}
\label{Background}

%--------------------------------------------------------------------------------------------------
\subsection{Building ATP Systems}
\label{Building}

ATP Systems are complex pieces of software, typically using advanced data structures~\cite{Sch13}, 
sophisticated algorithms~\cite{Vor01}, and tricky code optimizations\cite{Sch06}.
They are written in a variety of programming languages: Prolog~\cite{Ott23,Hol23}, 
Scala~\cite{SB18}, C~\cite{SCV19}, C++~\cite{RV02-AICOMM}, OCaml~\cite{Kor06}, Python~\cite{SP20}, 
etc.
Their build processes include techniques such as parser generators~\cite{Ste21}, Makefiles,
code repositories, specific versions of libraries, etc.
For a user who is focussed on an application of ATP
% , e.g. (with a few exemplar references), in 
% mathematics \cite{Qua92-Book,MP96}, logic~\cite{GO86,Jec95}, management~\cite{PB+92-TR,PM94}, 
% planning \cite{SE94}, 
installing an ATP system can be a deal breaker, and many early users selected a weaker system, 
e.g., Otter~\cite{McC03-Otter}, for their experiments because it was readily available and easy 
enough to install.
% As the TPTP World evolved it was clear that more powerful ATP systems were available, especially
% evident in the CADE ATP System Competition (CASC)~\cite{Sut16}.
% However, these more powerful systems were often not as easy to obtain and install.
% This was a key motivation for the creation of the SystemOnTPTP service~\cite{Sut00-CADE-17}.
% SystemOnTPTP allows users to
% The ATP systems in SystemOnTPTP are installed by the third author, often with help from the
% individual system developers.
There have been some proposals for standardising the ATP system build process, e.g.,
\href{https://tptp.org/Proposals/SystemBuild.html}{tptp.org/Proposals/SystemBuild.html}, 
but the diversity of ATP system software makes conformity nigh impossible.
An alternative is to push the task back on the system developers, and one approach to this is
containerization, as discussed in Section~\ref{Containerization}.

%--------------------------------------------------------------------------------------------------
\subsection{StarExec}
\label{StarExec}

The need to provide support for meaningful system evaluation has been recognized in many logic 
solver communities, e.g., TPTP~\cite{SS01}, SAT~\cite{HS00-SATLIB}, SMT~\cite{CSW15},
Termination~\cite{MZ07}, etc.
For many years testing of logic solvers was done on individual developer's computers. 
In 2010 a proposal for centralised hardware and software support was developed,
and in 2011 a \$2.11 million NSF grant\footnote{%
NSF Awards 1058748 and 1058925, led by Aaron Stump and Cesare Tinelli at the University of Iowa} 
was obtained.
This grant led to the development and availability of StarExec Iowa~\cite{SST14} in 2012,
and a subsequent \$1.00 million grant\footnote{%
NSF Award 1730419} in 2017 expanded StarExec to Miami.
StarExec has been central to much progress in logic solvers over the last 10 years, supporting
16 logic solver communities, used for running many annual competitions~\cite{BB+19}, and 
supporting many many users.

It was recently announced that StarExec Iowa will be decommissioned. 
The maintainer of StarExec Iowa explained that ``the plan is to operate StarExec as usual for 
competitions Summer 2024 and Summer 2025, and then put the system into a read-only mode for one 
year (Summer 2025 to Summer 2026)''.
While StarExec Miami will continue to operate while funding is available, but it will not be able
to support the large number of logic solver communities that use the larger StarExec Iowa cluster.
In the long run it will be necessary for StarExec users to transition to new environments,
and several plans are (at the time of writing) being discussed.
One effort is that described in the paper.

%--------------------------------------------------------------------------------------------------
\subsection{Containerization}
\label{Containerization}

{\em This section was written with the help of ChatGPT~3.5.}

Containerization is a technology that allows developers to package an application and its 
dependencies into a standardized unit called a container. 
These containers encapsulate the application code, runtime, libraries, and other necessary 
components, providing a consistent and isolated environment for running the application 
across different computing environments.
One of the key benefits of containerization is its ability to abstract away the underlying 
infrastructure. 
Containers are designed to be lightweight and portable, making it easy to deploy applications 
across various platforms, such as laptops, servers, virtual machines, and cloud environments. 
This portability ensures that applications behave consistently regardless of the underlying 
infrastructure, simplifying the development and deployment process.
Furthermore, containerization offers several advantages in terms of scalability, resource 
efficiency, and security. 
Containers share the host operating system's kernel, which reduces overhead compared to 
traditional virtualization techniques. 
As a result, containers can be started and stopped quickly, allowing for rapid scaling of 
applications to meet changing demand. 
Containers provide a level of isolation that helps prevent conflicts between applications and 
enhances security by limiting the impact of potential vulnerabilities.

Popular containerization platforms like Docker, Podman, and Kubernetes have played a significant 
role in popularizing container technology. 
These platforms provide tools for building, distributing, orchestrating, and managing containers 
at scale. 
Docker, for example, introduced a user-friendly interface and a standardized format for defining 
container images, making it easier for developers to adopt containerization in their workflow. 
Kubernetes, on the other hand, focuses on container orchestration and automating the deployment, 
scaling, and management of containerized applications in production environments.

%--------------------------------------------------------------------------------------------------
% Jack and Geoff and David
\section{Containerizing StarExec and ATP Systems}
\label{Containerizing}

Base build is Ubuntu.
The {\tt artifacts} directory, where all relevant executables are copied when an ATP system
is executed, is created as part of this container.

{\tt dockerhub} for storing containers.
{\tt tptpstarexec} repository.
One directory for StarExec, one directory for each ATP system.

%--------------------------------------------------------------------------------------------------
% David
\subsection{StarExec}
\label{StarExec}

%--------------------------------------------------------------------------------------------------
% Geoff and Jack
\subsection{ATP Systems}
\label{ATPSystems}

The {\tt tptp-world-build} container provides utilities from the TPTP World \cite{Sut24} that 
are used by ATP systems, e.g., {\tt SPCForProblem} detects the Specialist Problem Class (SPC) 
\cite{SS01} of a problem that is used by some ATP systems to decide on what search parameters 
to use.
Additionally, the {\tt runsolver} utility \cite{Rou11} for limiting and reporting the resources
used by an ATP system is part of this container.
To support these utilities some libraries that are not part of the {\tt ubuntu-build} have
to be added before downloading and building the 
The {\tt /benchmark} directory, where the TPTP problem for the ATP system to solve is placed, is
created as part of this container.

A runnable ATP system container is built in two phases.
The ATP system executables are first put into an {\em ATP-system-name}{\tt-build} container,
which is subsequently put into a {\tt -runsolver} container that adds the {\tt runsolver} utility 
so that the resource usage of the ATP system can be limited and reported.

The {\em ATP-system-name}{\tt-build} container with the ATP system's executables is based on the
{\tt ubuntu-build} container.
The ATP system is retrieved online, e.g., from a GitHub repository, and the necessary commands
to build the executables are run.
The executables are copied into the {\tt /artifacts} directory.
The choice of which version of the ATP system to containerize is made inside the {\tt Dockerfile},
e.g., in Figure~\ref{E---build} {\tt E 3.0.03} is chosen.
This localization is necessary because the incantations for selecting an ATP system version vary 
from system to system.
By convention the container is named {\em ATP-system-name}{\tt -build}, and by default has
the {\tt :latest} tag,
Figure~\ref{E---build} shows the {\tt Dockerfile} for building E's {\tt -build} container, using 
the command {\tt podman build -t eprover-build .}.

\begin{figure}[htb]
{\small
\begin{verbatim}
#------------------------------------------------------------
FROM ubuntu-build

# Clones repository
ARG E_VERSION=E-3.0.03
RUN git clone --depth 1 --branch $E_VERSION https://github.com/eprover/eprover.git

# Set working directory to cloned sources directory
WORKDIR /eprover

# Builds first-order executable
RUN ./configure --bindir=/artifacts && \
    make && \
    make install
# RUN cp PROVER/eprover /artifacts/eprover

# Builds higher-order executable
RUN ./configure --enable-ho && \
    make rebuild
RUN cp PROVER/eprover-ho /artifacts/eprover-ho
#------------------------------------------------------------
\end{verbatim}
}
\caption{The {\tt Dockerfile} for E's {\tt -build} container}
\label{E---build}
\end{figure}

The ATP system's {\tt -runsolver} container adds {\tt runsolver} control to the 
{\em ATP-system-name}{\tt-build} container, so that when executed the resource usage of the
ATP system can be limited and reported.
The container is based on the {\em ATP-system-name}{\tt-build} and {\tt tptp-world-build}
containers.
Note how the container is built with the default {\tt :latest} tagged 
{\em ATP-system-name}{\tt-build}. 
The executables from the {\tt -runsolver} container are copied from its {\tt /artifacts}
directory into this container's {\tt /artifacts} directory.
Additionally, the {\tt run\_container} script, described in Section~\ref{Running}, is copied into
{\tt /artifacts}.
By convention this container is named
{\em ATP-system-name}{\tt :}{\em ATP-system-version}{\tt -runsolver}, i.e., including the version
number so that users know what version of the ATP system has been containerized.
Figure~\ref{E---runsolver} shows the {\tt Dockerfile} for building E's {\tt -runsolver}
container, using the command {\tt podman build -t eprover:3.0.03-runsolver .}, i.e., it contains
E version {\tt 3.0.03}.

\begin{figure}[htb]
{\small
\begin{verbatim}
#------------------------------------------------------------
FROM eprover-build AS builder
FROM tptp-world-build

ENV PATH=".:${PATH}"
WORKDIR /artifacts

# E-specific stuff from ostensibly external image
COPY --from=builder /artifacts/eprover /artifacts/
COPY --from=builder /artifacts/eprover-ho /artifacts/

# run_container script 
ADD run_container /artifacts/

# run_E script 
ADD run_E /artifacts/

ENTRYPOINT ["runsolver"]
#------------------------------------------------------------
\end{verbatim}
}
\caption{The {\tt Dockerfile} for E's {\tt -runsolver} container}
\label{E---runsolver}
\end{figure}

Each {\tt -runsolver} container is pushed to dockerhub in it's repository directory, tagged
as {\em ATP-system-name}{\tt :}{\em ATP-system-version}{\tt -runsolver-}{\em architecture},
where {\em architecture} is, e.g., {\tt arm64} or {\tt amd64}.

%--------------------------------------------------------------------------------------------------
% Geoff 
\subsection{Running ATP System Containers}
\label{Running}

\begin{figure}[htb]
{\footnotesize
\begin{verbatim}
#------------------------------------------------------------
#!/usr/bin/env python3

import argparse
import subprocess
import os, sys
import shutil


def getRunsolverArgs(args):
    mem_part = f" -M {args.memory_limit}" if args.memory_limit > 0 else ""
    return f"--timestamp --watcher-data /dev/null -C {args.cpu_limit} -W {args.wall_clock_limit}{mem_part}"


def getRunscriptArgs(args, args_format):
    parts = {
        'P': "/artifacts/CWD/benchmark",
        'C': args.cpu_limit,
        'W': args.wall_clock_limit,
        'I': args.intent,
        'M': args.memory_limit,
    }
    return ' '.join([str(parts[c.upper()]) for c in args_format])

def makeBenchmark(problem):
    if problem:
        shutil.copy(problem, "./benchmark")
    else:
        with open('./benchmark', 'w') as benchmark:
            benchmark.write(sys.stdin.read())


if __name__ == "__main__":
    parser = argparse.ArgumentParser("Wrapper for a podman call to a prover image")
    parser.add_argument("image_name", help="Image name, e.g., eprover:3.0.03-runsolver-arm64")
    parser.add_argument("-P", "--problem", help="Problem file if not stdin")
    parser.add_argument("--runscript", default="run_container PCWMI", help="System script and its args, e.g., 'run_E PWI'")
    parser.add_argument("-C", "--cpu-limit", default=60, type=int, help="Max CPU time in seconds")
    parser.add_argument("-W", "--wall-clock-limit", default=60, type=int, help="Max wall clock time in seconds")
    parser.add_argument("-M", "--memory-limit", default=-1, type=int, help="Max memory usage in MB")
    parser.add_argument("-I", "--intent", default="THM", choices=["THM", "SAT"], help="Intention (THM, SAT, etc)")
    parser.add_argument("--dry-run", action="store_true", help="dry run")
    args = parser.parse_args()

    # Format arguments
    runsolverArgs = getRunsolverArgs(args)
    runscript, runscriptArgsFormat = args.runscript.split()
    runscriptArgs = getRunscriptArgs(args, runscriptArgsFormat)

    # Construct podman command
    command = f"podman run -v .:/artifacts/CWD -t {args.image_name} {runsolverArgs} {runscript} {runscriptArgs}"

    # Run command or print for dry run
    if args.dry_run:
        print(command)
    else:
        makeBenchmark(args.problem)
        subprocess.run(command, shell=True)
        os.remove("./benchmark")
#------------------------------------------------------------
\end{verbatim}
}
\caption{The {\tt runsystem.py} script}
\label{runsystem.py}
\end{figure}
%--------------------------------------------------------------------------------------------------
% Geoff and David
\section{StarExec with Kubernetes}
\label{StarExecK}

Figure~\ref{AWSArchitecture} shows the architecture of the final implemented system.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.8\textwidth]{AWSArchitecture}
\caption{AWS architecture}
\label{AWSArchitecture}
\end{center}
\end{figure}

%--------------------------------------------------------------------------------------------------
% Geoff
\section{Conclusion}
\label{Conclusion}

This paper 
%--------------------------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{Bibliography.bib}
%--------------------------------------------------------------------------------------------------
\end{document}
%--------------------------------------------------------------------------------------------------
